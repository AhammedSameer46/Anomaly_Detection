{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-27T14:45:03.365819Z",
     "start_time": "2026-01-27T14:45:02.377525Z"
    },
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2026-01-26T09:18:03.144216Z",
     "iopub.status.busy": "2026-01-26T09:18:03.143906Z",
     "iopub.status.idle": "2026-01-26T09:18:03.152840Z",
     "shell.execute_reply": "2026-01-26T09:18:03.151580Z",
     "shell.execute_reply.started": "2026-01-26T09:18:03.144190Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  ⁠Imports Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-27T14:46:12.254625Z",
     "start_time": "2026-01-27T14:46:11.801782Z"
    },
    "execution": {
     "iopub.execute_input": "2026-01-26T09:18:03.155309Z",
     "iopub.status.busy": "2026-01-26T09:18:03.154963Z",
     "iopub.status.idle": "2026-01-26T09:18:03.185434Z",
     "shell.execute_reply": "2026-01-26T09:18:03.184627Z",
     "shell.execute_reply.started": "2026-01-26T09:18:03.155283Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "ename": "XGBoostError",
     "evalue": "\nXGBoost Library (libxgboost.dylib) could not be loaded.\nLikely causes:\n  * OpenMP runtime is not installed\n    - vcomp140.dll or libgomp-1.dll for Windows\n    - libomp.dylib for Mac OSX\n    - libgomp.so for Linux and other UNIX-like OSes\n    Mac OSX users: Run `brew install libomp` to install OpenMP runtime.\n\n  * You are running 32-bit Python on a 64-bit OS\n\nError message(s): [\"dlopen(/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/xgboost/lib/libxgboost.dylib, 0x0006): Library not loaded: @rpath/libomp.dylib\\n  Referenced from: <58FE87DD-A5B4-3D80-BC4B-11FC831B9707> /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/xgboost/lib/libxgboost.dylib\\n  Reason: tried: '/opt/homebrew/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/opt/homebrew/opt/libomp/lib/libomp.dylib' (no such file), '/opt/homebrew/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/opt/homebrew/opt/libomp/lib/libomp.dylib' (no such file)\"]\n",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mXGBoostError\u001B[39m                              Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[6]\u001B[39m\u001B[32m, line 7\u001B[39m\n\u001B[32m      4\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01msklearn\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mmetrics\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m f1_score, classification_report\n\u001B[32m      5\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01msklearn\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mimpute\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m SimpleImputer\n\u001B[32m----> \u001B[39m\u001B[32m7\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mxgboost\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m XGBClassifier\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/xgboost/__init__.py:6\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[33;03m\"\"\"XGBoost: eXtreme Gradient Boosting library.\u001B[39;00m\n\u001B[32m      2\u001B[39m \n\u001B[32m      3\u001B[39m \u001B[33;03mContributors: https://github.com/dmlc/xgboost/blob/master/CONTRIBUTORS.md\u001B[39;00m\n\u001B[32m      4\u001B[39m \u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m6\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m.\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m tracker  \u001B[38;5;66;03m# noqa\u001B[39;00m\n\u001B[32m      7\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m.\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m collective\n\u001B[32m      8\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m.\u001B[39;00m\u001B[34;01mcore\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[32m      9\u001B[39m     Booster,\n\u001B[32m     10\u001B[39m     DataIter,\n\u001B[32m   (...)\u001B[39m\u001B[32m     15\u001B[39m     build_info,\n\u001B[32m     16\u001B[39m )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/xgboost/tracker.py:9\u001B[39m\n\u001B[32m      6\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01menum\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m IntEnum, unique\n\u001B[32m      7\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtyping\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m Dict, Optional, Union\n\u001B[32m----> \u001B[39m\u001B[32m9\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m.\u001B[39;00m\u001B[34;01mcore\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m _LIB, _check_call, _deprecate_positional_args, make_jcargs\n\u001B[32m     12\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mget_family\u001B[39m(addr: \u001B[38;5;28mstr\u001B[39m) -> \u001B[38;5;28mint\u001B[39m:\n\u001B[32m     13\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"Get network family from address.\"\"\"\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/xgboost/core.py:308\u001B[39m\n\u001B[32m    304\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m lib\n\u001B[32m    307\u001B[39m \u001B[38;5;66;03m# load the XGBoost library globally\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m308\u001B[39m _LIB = \u001B[43m_load_lib\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    311\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_check_call\u001B[39m(ret: \u001B[38;5;28mint\u001B[39m) -> \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    312\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"Check the return value of C API call\u001B[39;00m\n\u001B[32m    313\u001B[39m \n\u001B[32m    314\u001B[39m \u001B[33;03m    This function will raise exception when error occurs.\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    320\u001B[39m \u001B[33;03m        return value from API calls\u001B[39;00m\n\u001B[32m    321\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/xgboost/core.py:270\u001B[39m, in \u001B[36m_load_lib\u001B[39m\u001B[34m()\u001B[39m\n\u001B[32m    268\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m lib_success:\n\u001B[32m    269\u001B[39m         libname = os.path.basename(lib_paths[\u001B[32m0\u001B[39m])\n\u001B[32m--> \u001B[39m\u001B[32m270\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m XGBoostError(\n\u001B[32m    271\u001B[39m             \u001B[33mf\u001B[39m\u001B[33m\"\"\"\u001B[39m\n\u001B[32m    272\u001B[39m \u001B[33mXGBoost Library (\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mlibname\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m) could not be loaded.\u001B[39m\n\u001B[32m    273\u001B[39m \u001B[33mLikely causes:\u001B[39m\n\u001B[32m    274\u001B[39m \u001B[33m  * OpenMP runtime is not installed\u001B[39m\n\u001B[32m    275\u001B[39m \u001B[33m    - vcomp140.dll or libgomp-1.dll for Windows\u001B[39m\n\u001B[32m    276\u001B[39m \u001B[33m    - libomp.dylib for Mac OSX\u001B[39m\n\u001B[32m    277\u001B[39m \u001B[33m    - libgomp.so for Linux and other UNIX-like OSes\u001B[39m\n\u001B[32m    278\u001B[39m \u001B[33m    Mac OSX users: Run `brew install libomp` to install OpenMP runtime.\u001B[39m\n\u001B[32m    279\u001B[39m \n\u001B[32m    280\u001B[39m \u001B[33m  * You are running 32-bit Python on a 64-bit OS\u001B[39m\n\u001B[32m    281\u001B[39m \n\u001B[32m    282\u001B[39m \u001B[33mError message(s): \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mos_error_list\u001B[38;5;132;01m}\u001B[39;00m\n\u001B[32m    283\u001B[39m \u001B[33m\"\"\"\u001B[39m\n\u001B[32m    284\u001B[39m         )\n\u001B[32m    285\u001B[39m     _register_log_callback(lib)\n\u001B[32m    287\u001B[39m     libver = _lib_version(lib)\n",
      "\u001B[31mXGBoostError\u001B[39m: \nXGBoost Library (libxgboost.dylib) could not be loaded.\nLikely causes:\n  * OpenMP runtime is not installed\n    - vcomp140.dll or libgomp-1.dll for Windows\n    - libomp.dylib for Mac OSX\n    - libgomp.so for Linux and other UNIX-like OSes\n    Mac OSX users: Run `brew install libomp` to install OpenMP runtime.\n\n  * You are running 32-bit Python on a 64-bit OS\n\nError message(s): [\"dlopen(/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/xgboost/lib/libxgboost.dylib, 0x0006): Library not loaded: @rpath/libomp.dylib\\n  Referenced from: <58FE87DD-A5B4-3D80-BC4B-11FC831B9707> /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/xgboost/lib/libxgboost.dylib\\n  Reason: tried: '/opt/homebrew/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/opt/homebrew/opt/libomp/lib/libomp.dylib' (no such file), '/opt/homebrew/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/opt/homebrew/opt/libomp/lib/libomp.dylib' (no such file)\"]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⁠Load Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-27T14:47:03.141560Z",
     "start_time": "2026-01-27T14:47:03.085922Z"
    },
    "execution": {
     "iopub.execute_input": "2026-01-26T09:18:03.186923Z",
     "iopub.status.busy": "2026-01-26T09:18:03.186681Z",
     "iopub.status.idle": "2026-01-26T09:18:03.399202Z",
     "shell.execute_reply": "2026-01-26T09:18:03.397770Z",
     "shell.execute_reply.started": "2026-01-26T09:18:03.186899Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Unable to find a usable engine; tried using: 'pyarrow', 'fastparquet'.\nA suitable version of pyarrow or fastparquet is required for parquet support.\nTrying to import the above resulted in these errors:\n - `Import pyarrow` failed. pyarrow is required for parquet support. Use pip or conda to install the pyarrow package.\n - `Import fastparquet` failed. fastparquet is required for parquet support. Use pip or conda to install the fastparquet package.",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mImportError\u001B[39m                               Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[7]\u001B[39m\u001B[32m, line 4\u001B[39m\n\u001B[32m      1\u001B[39m train_path = \u001B[33m\"\u001B[39m\u001B[33m/kaggle/input/ana-verse-2-0-j/train.parquet\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m      2\u001B[39m test_path  = \u001B[33m\"\u001B[39m\u001B[33m/kaggle/input/ana-verse-2-0-j/test.parquet\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m4\u001B[39m train_df = \u001B[43mpd\u001B[49m\u001B[43m.\u001B[49m\u001B[43mread_parquet\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_path\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m      5\u001B[39m test_df  = pd.read_parquet(test_path)\n\u001B[32m      7\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33mTrain shape:\u001B[39m\u001B[33m\"\u001B[39m, train_df.shape)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pandas/io/parquet.py:668\u001B[39m, in \u001B[36mread_parquet\u001B[39m\u001B[34m(path, engine, columns, storage_options, dtype_backend, filesystem, filters, to_pandas_kwargs, **kwargs)\u001B[39m\n\u001B[32m    508\u001B[39m \u001B[38;5;129m@set_module\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33mpandas\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m    509\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mread_parquet\u001B[39m(\n\u001B[32m    510\u001B[39m     path: FilePath | ReadBuffer[\u001B[38;5;28mbytes\u001B[39m],\n\u001B[32m   (...)\u001B[39m\u001B[32m    518\u001B[39m     **kwargs,\n\u001B[32m    519\u001B[39m ) -> DataFrame:\n\u001B[32m    520\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    521\u001B[39m \u001B[33;03m    Load a parquet object from the file path, returning a DataFrame.\u001B[39;00m\n\u001B[32m    522\u001B[39m \n\u001B[32m   (...)\u001B[39m\u001B[32m    665\u001B[39m \u001B[33;03m    1    4    9\u001B[39;00m\n\u001B[32m    666\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m668\u001B[39m     impl = \u001B[43mget_engine\u001B[49m\u001B[43m(\u001B[49m\u001B[43mengine\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    669\u001B[39m     check_dtype_backend(dtype_backend)\n\u001B[32m    671\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m impl.read(\n\u001B[32m    672\u001B[39m         path,\n\u001B[32m    673\u001B[39m         columns=columns,\n\u001B[32m   (...)\u001B[39m\u001B[32m    679\u001B[39m         **kwargs,\n\u001B[32m    680\u001B[39m     )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pandas/io/parquet.py:68\u001B[39m, in \u001B[36mget_engine\u001B[39m\u001B[34m(engine)\u001B[39m\n\u001B[32m     65\u001B[39m         \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n\u001B[32m     66\u001B[39m             error_msgs += \u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m - \u001B[39m\u001B[33m\"\u001B[39m + \u001B[38;5;28mstr\u001B[39m(err)\n\u001B[32m---> \u001B[39m\u001B[32m68\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m(\n\u001B[32m     69\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mUnable to find a usable engine; \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m     70\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mtried using: \u001B[39m\u001B[33m'\u001B[39m\u001B[33mpyarrow\u001B[39m\u001B[33m'\u001B[39m\u001B[33m, \u001B[39m\u001B[33m'\u001B[39m\u001B[33mfastparquet\u001B[39m\u001B[33m'\u001B[39m\u001B[33m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m\n\u001B[32m     71\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mA suitable version of \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m     72\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mpyarrow or fastparquet is required for parquet \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m     73\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33msupport.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m\n\u001B[32m     74\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mTrying to import the above resulted in these errors:\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m     75\u001B[39m         \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00merror_msgs\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m\n\u001B[32m     76\u001B[39m     )\n\u001B[32m     78\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m engine == \u001B[33m\"\u001B[39m\u001B[33mpyarrow\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m     79\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m PyArrowImpl()\n",
      "\u001B[31mImportError\u001B[39m: Unable to find a usable engine; tried using: 'pyarrow', 'fastparquet'.\nA suitable version of pyarrow or fastparquet is required for parquet support.\nTrying to import the above resulted in these errors:\n - `Import pyarrow` failed. pyarrow is required for parquet support. Use pip or conda to install the pyarrow package.\n - `Import fastparquet` failed. fastparquet is required for parquet support. Use pip or conda to install the fastparquet package."
     ]
    }
   ],
   "source": [
    "train_path = \"/kaggle/input/ana-verse-2-0-j/train.parquet\"\n",
    "test_path  = \"/kaggle/input/ana-verse-2-0-j/test.parquet\"\n",
    "\n",
    "train_df = pd.read_parquet(train_path)\n",
    "test_df  = pd.read_parquet(test_path)\n",
    "\n",
    "print(\"Train shape:\", train_df.shape)\n",
    "print(\"Test shape:\", test_df.shape)\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T09:18:03.401593Z",
     "iopub.status.busy": "2026-01-26T09:18:03.400901Z",
     "iopub.status.idle": "2026-01-26T09:18:03.412807Z",
     "shell.execute_reply": "2026-01-26T09:18:03.411452Z",
     "shell.execute_reply.started": "2026-01-26T09:18:03.401561Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test_df.head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  ⁠Basic Inspection\n",
    "##### This step is used to understand the structure of the dataset, including data types, column names, and memory usage, before applying any preprocessing or modeling steps.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-27T14:47:08.742780Z",
     "start_time": "2026-01-27T14:47:08.722132Z"
    },
    "execution": {
     "iopub.execute_input": "2026-01-26T09:18:03.416170Z",
     "iopub.status.busy": "2026-01-26T09:18:03.415183Z",
     "iopub.status.idle": "2026-01-26T09:18:03.543178Z",
     "shell.execute_reply": "2026-01-26T09:18:03.542263Z",
     "shell.execute_reply.started": "2026-01-26T09:18:03.416138Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mNameError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[8]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m \u001B[43mtrain_df\u001B[49m.info()\n\u001B[32m      2\u001B[39m train_df.columns\n",
      "\u001B[31mNameError\u001B[39m: name 'train_df' is not defined"
     ]
    }
   ],
   "source": [
    "train_df.info()\n",
    "train_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target Variable Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T09:18:03.544544Z",
     "iopub.status.busy": "2026-01-26T09:18:03.544203Z",
     "iopub.status.idle": "2026-01-26T09:18:03.716887Z",
     "shell.execute_reply": "2026-01-26T09:18:03.715601Z",
     "shell.execute_reply.started": "2026-01-26T09:18:03.544507Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_df['target'] = train_df['target'].astype(int)\n",
    "train_df['target'].dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target Distribution (Imbalance Check)\n",
    "##### The target variable is highly imbalanced, with anomalies forming a very small fraction of the data.  \n",
    "##### This confirms the need for F1 score as the evaluation metric and explicit handling of class imbalance during modeling.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T09:18:03.719416Z",
     "iopub.status.busy": "2026-01-26T09:18:03.718378Z",
     "iopub.status.idle": "2026-01-26T09:18:03.748628Z",
     "shell.execute_reply": "2026-01-26T09:18:03.747318Z",
     "shell.execute_reply.started": "2026-01-26T09:18:03.719370Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_df['target'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⁠ ⁠Sort by Time\n",
    "##### The dataset is sorted chronologically to preserve the natural order of events.  \n",
    "##### This helps prevent information leakage from future observations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T09:18:03.750513Z",
     "iopub.status.busy": "2026-01-26T09:18:03.750053Z",
     "iopub.status.idle": "2026-01-26T09:18:04.231677Z",
     "shell.execute_reply": "2026-01-26T09:18:04.230600Z",
     "shell.execute_reply.started": "2026-01-26T09:18:03.750456Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_df['Date'] = pd.to_datetime(train_df['Date'])\n",
    "test_df['Date']  = pd.to_datetime(test_df['Date'])\n",
    "\n",
    "train_df = train_df.sort_values(\"Date\").reset_index(drop=True)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⁠Time-Based Train–Validation Split\n",
    "##### The training data is split using time order rather than random sampling.  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T09:18:04.233555Z",
     "iopub.status.busy": "2026-01-26T09:18:04.232886Z",
     "iopub.status.idle": "2026-01-26T09:18:04.382727Z",
     "shell.execute_reply": "2026-01-26T09:18:04.381780Z",
     "shell.execute_reply.started": "2026-01-26T09:18:04.233517Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2026-01-30T06:10:26.093654Z",
     "start_time": "2026-01-30T06:10:26.006223Z"
    }
   },
   "source": [
    "split_date = train_df['Date'].quantile(0.8)\n",
    "\n",
    "train_data = train_df[train_df['Date'] <= split_date]\n",
    "val_data   = train_df[train_df['Date'] > split_date]\n",
    "\n",
    "X_train = train_data.drop(columns=['target'])\n",
    "y_train = train_data['target']\n",
    "\n",
    "X_val = val_data.drop(columns=['target'])\n",
    "y_val = val_data['target']\n",
    "\n",
    "X_test = test_df.copy()\n",
    "test_df.head()"
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mNameError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[1]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m split_date = \u001B[43mtrain_df\u001B[49m[\u001B[33m'\u001B[39m\u001B[33mDate\u001B[39m\u001B[33m'\u001B[39m].quantile(\u001B[32m0.8\u001B[39m)\n\u001B[32m      3\u001B[39m train_data = train_df[train_df[\u001B[33m'\u001B[39m\u001B[33mDate\u001B[39m\u001B[33m'\u001B[39m] <= split_date]\n\u001B[32m      4\u001B[39m val_data   = train_df[train_df[\u001B[33m'\u001B[39m\u001B[33mDate\u001B[39m\u001B[33m'\u001B[39m] > split_date]\n",
      "\u001B[31mNameError\u001B[39m: name 'train_df' is not defined"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⁠Time Feature Engineering\n",
    "##### Basic calendar features are extracted from the timestamp to capture periodic patterns such as weekday and monthly behavior, without introducing excessive complexity.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T09:18:04.384729Z",
     "iopub.status.busy": "2026-01-26T09:18:04.384321Z",
     "iopub.status.idle": "2026-01-26T09:18:04.686794Z",
     "shell.execute_reply": "2026-01-26T09:18:04.685601Z",
     "shell.execute_reply.started": "2026-01-26T09:18:04.384691Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def add_time_features(df):\n",
    "    df = df.copy()\n",
    "    df['day'] = df['Date'].dt.day\n",
    "    df['month'] = df['Date'].dt.month\n",
    "    df['dayofweek'] = df['Date'].dt.dayofweek\n",
    "    df.drop(columns=['Date'], inplace=True)\n",
    "    return df\n",
    "\n",
    "X_train = add_time_features(X_train)\n",
    "X_val   = add_time_features(X_val)\n",
    "X_test  = add_time_features(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T09:18:04.688513Z",
     "iopub.status.busy": "2026-01-26T09:18:04.688096Z",
     "iopub.status.idle": "2026-01-26T09:18:04.701983Z",
     "shell.execute_reply": "2026-01-26T09:18:04.700939Z",
     "shell.execute_reply.started": "2026-01-26T09:18:04.688448Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T09:18:04.704044Z",
     "iopub.status.busy": "2026-01-26T09:18:04.703371Z",
     "iopub.status.idle": "2026-01-26T09:18:04.733658Z",
     "shell.execute_reply": "2026-01-26T09:18:04.732557Z",
     "shell.execute_reply.started": "2026-01-26T09:18:04.703967Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "X_val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T09:18:04.735553Z",
     "iopub.status.busy": "2026-01-26T09:18:04.734858Z",
     "iopub.status.idle": "2026-01-26T09:18:04.761997Z",
     "shell.execute_reply": "2026-01-26T09:18:04.760902Z",
     "shell.execute_reply.started": "2026-01-26T09:18:04.735509Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "X_test.head() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rolling Features \n",
    "##### Short-term rolling averages are added to capture recent sensor behavior.  \n",
    "##### This helps the model detect sudden spikes or shifts relative to recent readings, which is common in anomaly scenarios.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-27T14:43:54.440578Z",
     "start_time": "2026-01-27T14:43:54.353211Z"
    },
    "execution": {
     "iopub.execute_input": "2026-01-26T09:18:04.766401Z",
     "iopub.status.busy": "2026-01-26T09:18:04.766062Z",
     "iopub.status.idle": "2026-01-26T09:18:05.235910Z",
     "shell.execute_reply": "2026-01-26T09:18:05.234958Z",
     "shell.execute_reply.started": "2026-01-26T09:18:04.766371Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mNameError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[1]\u001B[39m\u001B[32m, line 5\u001B[39m\n\u001B[32m      2\u001B[39m WINDOW = \u001B[32m3\u001B[39m\n\u001B[32m      4\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m col \u001B[38;5;129;01min\u001B[39;00m [\u001B[33m'\u001B[39m\u001B[33mX1\u001B[39m\u001B[33m'\u001B[39m, \u001B[33m'\u001B[39m\u001B[33mX2\u001B[39m\u001B[33m'\u001B[39m, \u001B[33m'\u001B[39m\u001B[33mX3\u001B[39m\u001B[33m'\u001B[39m, \u001B[33m'\u001B[39m\u001B[33mX4\u001B[39m\u001B[33m'\u001B[39m, \u001B[33m'\u001B[39m\u001B[33mX5\u001B[39m\u001B[33m'\u001B[39m]:\n\u001B[32m----> \u001B[39m\u001B[32m5\u001B[39m     X_train[\u001B[33mf\u001B[39m\u001B[33m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mcol\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m_roll_mean\u001B[39m\u001B[33m'\u001B[39m] = \u001B[43mX_train\u001B[49m[col].rolling(WINDOW).mean()\n\u001B[32m      6\u001B[39m     X_val[\u001B[33mf\u001B[39m\u001B[33m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mcol\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m_roll_mean\u001B[39m\u001B[33m'\u001B[39m]   = X_val[col].rolling(WINDOW).mean()\n\u001B[32m      7\u001B[39m     X_test[\u001B[33mf\u001B[39m\u001B[33m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mcol\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m_roll_mean\u001B[39m\u001B[33m'\u001B[39m]  = X_test[col].rolling(WINDOW).mean()\n",
      "\u001B[31mNameError\u001B[39m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "# Rolling Features\n",
    "WINDOW = 3\n",
    "\n",
    "for col in ['X1', 'X2', 'X3', 'X4', 'X5']:\n",
    "    X_train[f'{col}_roll_mean'] = X_train[col].rolling(WINDOW).mean()\n",
    "    X_val[f'{col}_roll_mean']   = X_val[col].rolling(WINDOW).mean()\n",
    "    X_test[f'{col}_roll_mean']  = X_test[col].rolling(WINDOW).mean()\n",
    "\n",
    "# Handle NaNs introduced by rolling windows\n",
    "X_train = X_train.bfill()\n",
    "X_val   = X_val.bfill()\n",
    "X_test  = X_test.bfill()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Value Handling\n",
    "##### Median imputation is applied to ensure robustness against missing or corrupted sensor readings.  \n",
    "##### Although the current dataset has no missing values, this step makes the pipeline resilient for real-world data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T09:18:05.237762Z",
     "iopub.status.busy": "2026-01-26T09:18:05.237356Z",
     "iopub.status.idle": "2026-01-26T09:18:08.899868Z",
     "shell.execute_reply": "2026-01-26T09:18:08.898937Z",
     "shell.execute_reply.started": "2026-01-26T09:18:05.237706Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# drop non-feature columns if present(ID)\n",
    "for df in [X_train, X_val, X_test]:\n",
    "    if 'ID' in df.columns:\n",
    "        df.drop(columns=['ID'], inplace=True)\n",
    "\n",
    "# Median imputation\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "\n",
    "X_train = pd.DataFrame(\n",
    "    imputer.fit_transform(X_train),\n",
    "    columns=X_train.columns,\n",
    "    index=X_train.index\n",
    ")\n",
    "\n",
    "X_val = pd.DataFrame(\n",
    "    imputer.transform(X_val),\n",
    "    columns=X_train.columns,\n",
    "    index=X_val.index\n",
    ")\n",
    "\n",
    "X_test = pd.DataFrame(\n",
    "    imputer.transform(X_test),\n",
    "    columns=X_train.columns,\n",
    "    index=X_test.index\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⁠Model Setup (XGBoost with Imbalance Handling)\n",
    "##### XGBoost is used due to its strong performance on tabular data.  \n",
    "##### Class imbalance is handled by assigning higher weight to anomaly samples, ensuring the model does not become biased toward normal observations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-27T14:43:29.084523Z",
     "start_time": "2026-01-27T14:43:29.064250Z"
    },
    "execution": {
     "iopub.execute_input": "2026-01-26T09:18:08.901286Z",
     "iopub.status.busy": "2026-01-26T09:18:08.901027Z",
     "iopub.status.idle": "2026-01-26T09:18:28.244222Z",
     "shell.execute_reply": "2026-01-26T09:18:28.242524Z",
     "shell.execute_reply.started": "2026-01-26T09:18:08.901262Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mNameError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[2]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m pos_weight = (\u001B[43my_train\u001B[49m == \u001B[32m0\u001B[39m).sum() / (y_train == \u001B[32m1\u001B[39m).sum()\n\u001B[32m      3\u001B[39m xgb_model = XGBClassifier(\n\u001B[32m      4\u001B[39m     n_estimators=\u001B[32m400\u001B[39m,\n\u001B[32m      5\u001B[39m     max_depth=\u001B[32m6\u001B[39m,\n\u001B[32m   (...)\u001B[39m\u001B[32m     12\u001B[39m     n_jobs=-\u001B[32m1\u001B[39m\n\u001B[32m     13\u001B[39m )\n\u001B[32m     15\u001B[39m xgb_model.fit(X_train, y_train)\n",
      "\u001B[31mNameError\u001B[39m: name 'y_train' is not defined"
     ]
    }
   ],
   "source": [
    "pos_weight = (y_train == 0).sum() / (y_train == 1).sum()\n",
    "\n",
    "xgb_model = XGBClassifier(\n",
    "    n_estimators=400,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.05,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    scale_pos_weight=pos_weight,\n",
    "    eval_metric='logloss',\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⁠Validation Evaluation\n",
    "##### Multiple probability thresholds were evaluated to identify the value that maximizes the F1 score on validation data.  \n",
    "##### The optimal threshold was found to be 0.8, indicating that high-confidence anomaly predictions provide the best balance between precision and recall for this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T09:18:28.245247Z",
     "iopub.status.busy": "2026-01-26T09:18:28.245001Z",
     "iopub.status.idle": "2026-01-26T09:18:31.898323Z",
     "shell.execute_reply": "2026-01-26T09:18:31.897226Z",
     "shell.execute_reply.started": "2026-01-26T09:18:28.245223Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "val_probs = xgb_model.predict_proba(X_val)[:, 1]\n",
    "\n",
    "#Threshold Optimization (for F1)\n",
    "\n",
    "thresholds = np.linspace(0.05, 0.8, 60)\n",
    "f1_scores = []\n",
    "\n",
    "for t in thresholds:\n",
    "    preds = (val_probs >= t).astype(int)\n",
    "    f1_scores.append(f1_score(y_val, preds))\n",
    "\n",
    "best_threshold = thresholds[np.argmax(f1_scores)]\n",
    "best_f1 = max(f1_scores)\n",
    "\n",
    "print(\"Best Threshold:\", best_threshold)\n",
    "print(\"Best F1 Score:\", best_f1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⁠ ⁠Retrain on Full Training Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T09:18:31.900576Z",
     "iopub.status.busy": "2026-01-26T09:18:31.900186Z",
     "iopub.status.idle": "2026-01-26T09:18:55.604515Z",
     "shell.execute_reply": "2026-01-26T09:18:55.603776Z",
     "shell.execute_reply.started": "2026-01-26T09:18:31.900538Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "full_X = pd.concat([X_train, X_val])\n",
    "full_y = pd.concat([y_train, y_val])\n",
    "\n",
    "xgb_model.fit(full_X, full_y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⁠Test Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T09:18:55.607059Z",
     "iopub.status.busy": "2026-01-26T09:18:55.606099Z",
     "iopub.status.idle": "2026-01-26T09:18:58.204363Z",
     "shell.execute_reply": "2026-01-26T09:18:58.203701Z",
     "shell.execute_reply.started": "2026-01-26T09:18:55.607025Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test_probs = xgb_model.predict_proba(X_test)[:, 1]\n",
    "test_preds = (test_probs >= best_threshold).astype(int)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T09:18:58.206835Z",
     "iopub.status.busy": "2026-01-26T09:18:58.206524Z",
     "iopub.status.idle": "2026-01-26T09:18:58.212814Z",
     "shell.execute_reply": "2026-01-26T09:18:58.211000Z",
     "shell.execute_reply.started": "2026-01-26T09:18:58.206806Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"Test rows:\", test_df.shape[0])\n",
    "print(\"Predictions:\", len(test_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T09:18:58.214000Z",
     "iopub.status.busy": "2026-01-26T09:18:58.213761Z",
     "iopub.status.idle": "2026-01-26T09:18:58.234924Z",
     "shell.execute_reply": "2026-01-26T09:18:58.233945Z",
     "shell.execute_reply.started": "2026-01-26T09:18:58.213976Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"Total predictions:\", len(test_preds))\n",
    "print(\"Anomalies predicted:\", np.sum(test_preds))\n",
    "print(\"Anomaly ratio:\", np.mean(test_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead write this below:\n",
    "\n",
    "\n",
    "##### Predictions were generated for 409,856 test samples after retraining the XGBoost model on all available data.\n",
    "##### Anomaly detection used an optimized probability threshold of 0.8, flagging 15,651 samples (~3.8%) as anomalous.\n",
    "##### Validation achieved an F1 score of 0.5144, indicating a stable precision–recall balance under severe class imbalance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⁠Create Submission File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T09:23:45.384855Z",
     "iopub.status.busy": "2026-01-26T09:23:45.384452Z",
     "iopub.status.idle": "2026-01-26T09:23:45.549797Z",
     "shell.execute_reply": "2026-01-26T09:23:45.548446Z",
     "shell.execute_reply.started": "2026-01-26T09:23:45.384823Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Create Submission File (CSV)\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    'target': test_preds\n",
    "})\n",
    "\n",
    "submission.to_csv(\"submission.csv\", index=False)\n",
    "\n",
    "submission.head()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 15414475,
     "sourceId": 128577,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31259,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
